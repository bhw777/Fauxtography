{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Colorspace EfficientNet (MC-EffNet)\n",
    "> Reference: [Distinguishing Natural and Computer-Generated Images using Multi-Colorspace fused EfficientNet](https://arxiv.org/pdf/2110.09428.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from skimage.color import rgb2lab, lab2lch, rgb2hsv\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCH = 5\n",
    "BS = 32\n",
    "LR = 1e-4\n",
    "NUM_CLASSES = 2\n",
    "MODEL = 'efficientnet-b0'\n",
    "LEAST_NUM_TRAIN_DATA = 6000\n",
    "LEAST_NUM_VAL_DATA = 6000\n",
    "INPUT_IMG_SIZE = (128, 128)\n",
    "\n",
    "# ----- Original Fake2M dataset -----\n",
    "mdl_pth = f'./model/efficientnet_model_MC_Fake2M.pth'\n",
    "rst_pth = f'./model/efficientnet_rst_MC_Fake2M.pth'\n",
    "train_data_pth = './Fake2M/train/'\n",
    "val_data_pth = './Fake2M/val/'\n",
    "\n",
    "# ----- Random compressed and cropped Fake2M dataset -----\n",
    "# mdl_pth = f'./model/efficientnet_model_MC_Fake2M_cc.pth'\n",
    "# rst_pth = f'./model/efficientnet_rst_MC_Fake2M_cc.pth'\n",
    "# train_data_pth = './Fake2M_cc/train/'\n",
    "# val_data_pth = './Fake2M_cc/val/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize0to1(image):\n",
    "    modified_img = image\n",
    "    min_1 = np.min(modified_img[:,:,0])\n",
    "    max_1 = np.max(modified_img[:,:,0])\n",
    "    modified_img[:,:,0] = np.minimum((modified_img[:,:,0] - min_1) / (max_1 - min_1 + 1e-8), 1.0)\n",
    "    min_2 = np.min(modified_img[:,:,1])\n",
    "    max_2 = np.max(modified_img[:,:,1])\n",
    "    modified_img[:,:,1] = np.minimum((modified_img[:,:,1] - min_2) / (max_2 - min_2 + 1e-8), 1.0)\n",
    "    min_3 = np.min(modified_img[:,:,2])\n",
    "    max_3 = np.max(modified_img[:,:,2])\n",
    "    modified_img[:,:,2] = np.minimum((modified_img[:,:,2] - min_3) / (max_3 - min_3 + 1e-8), 1.0)\n",
    "    return modified_img\n",
    "\n",
    "def rgb_to_hsv(image):\n",
    "    hsv_image = rgb2hsv(image)\n",
    "    hsv_image = normalize0to1(hsv_image)\n",
    "    return hsv_image\n",
    "\n",
    "def rgb_to_lch(image):\n",
    "    lab_image = rgb2lab(image)\n",
    "    lch_image = lab2lch(lab_image)\n",
    "    lch_image = normalize0to1(lch_image)\n",
    "    np.nan_to_num(lch_image, copy=False, nan=0.0, posinf=None, neginf=None)\n",
    "    return lch_image\n",
    "\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, rgb_tranform, hsv_tranform, lch_tranform):\n",
    "        self.rgb_dataset = ImageFolder(root, transform=rgb_tranform)\n",
    "        self.hsv_dataset = ImageFolder(root, transform=hsv_tranform)\n",
    "        self.lch_dataset = ImageFolder(root, transform=lch_tranform)\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        rgb_data, labels = self.rgb_dataset[index]\n",
    "        hsv_data, _ = self.hsv_dataset[index]\n",
    "        lch_data, _ = self.lch_dataset[index]\n",
    "\n",
    "        return rgb_data, hsv_data, lch_data, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rgb_dataset)\n",
    "\n",
    "class EfficientNetWrapper(nn.Module):\n",
    "    def __init__(self, num_classes=1280):\n",
    "        super(EfficientNetWrapper, self).__init__()\n",
    "        self.rgb_model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "        self.hsv_model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "        self.lch_model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "\n",
    "        # remove last layer\n",
    "        self.rgb_model._fc = nn.Identity()\n",
    "        self.hsv_model._fc = nn.Identity()\n",
    "        self.lch_model._fc = nn.Identity()\n",
    "\n",
    "        # concat\n",
    "        self.fc = nn.Linear(3 * num_classes, 2)\n",
    "\n",
    "    def forward(self, rgb_input, hsv_input, lch_input):\n",
    "        rgb_output = self.rgb_model(rgb_input)\n",
    "        hsv_output = self.hsv_model(hsv_input)\n",
    "        lch_output = self.lch_model(lch_input)\n",
    "\n",
    "        # concat output\n",
    "        concatenated_output = torch.cat([rgb_output, hsv_output, lch_output], dim=1)\n",
    "\n",
    "        # final output\n",
    "        final_output = self.fc(concatenated_output)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_transform = transforms.Compose([\n",
    "    transforms.Resize(INPUT_IMG_SIZE),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "hsv_transform = transforms.Compose([\n",
    "    transforms.Resize(INPUT_IMG_SIZE),\n",
    "    transforms.Lambda(lambda x: rgb_to_hsv(np.array(x))),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "lch_transform = transforms.Compose([\n",
    "    transforms.Resize(INPUT_IMG_SIZE),\n",
    "    transforms.Lambda(lambda x: rgb_to_lch(np.array(x))),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = CustomDataset(\n",
    "    train_data_pth,\n",
    "    rgb_tranform=rgb_transform,\n",
    "    hsv_tranform=hsv_transform,\n",
    "    lch_tranform=lch_transform\n",
    ")\n",
    "val_dataset = CustomDataset(\n",
    "    val_data_pth,\n",
    "    rgb_tranform=rgb_transform,\n",
    "    hsv_tranform=hsv_transform,\n",
    "    lch_tranform=lch_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = SubsetRandomSampler(range(0, len(train_dataset), len(train_dataset) // LEAST_NUM_TRAIN_DATA)) \n",
    "val_sampler = SubsetRandomSampler(range(0, len(val_dataset), len(val_dataset) // LEAST_NUM_VAL_DATA))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BS, sampler=train_sampler)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BS, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training data: 6944\n",
      "Number of validation data: 6784\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training data: {len(train_dataloader) * train_dataloader.batch_size}')\n",
    "print(f'Number of validation data: {len(val_dataloader) * val_dataloader.batch_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGB range: (0.007843137718737125, 0.9960784316062927)\n",
      "HSV range: (0.0, 0.9999999899354839)\n",
      "LCH range: (0.0, 0.9999999998899461)\n"
     ]
    }
   ],
   "source": [
    "for rgb, hsv, lch, labels in train_dataloader:\n",
    "    rgb_img = rgb[0]\n",
    "    hsv_img = hsv[0]\n",
    "    lch_img = lch[0]\n",
    "    print(f'RGB range: ({torch.min(rgb_img)}, {torch.max(rgb_img)})')\n",
    "    print(f'HSV range: ({torch.min(hsv_img)}, {torch.max(hsv_img)})')\n",
    "    print(f'LCH range: ({torch.min(lch_img)}, {torch.max(lch_img)})')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of data in each class\n",
    "check_data_label = False \n",
    "if check_data_label:\n",
    "    train_lbl_0 = 0\n",
    "    train_lbl_1 = 0\n",
    "    for _, _, _, labels in tqdm(train_dataloader):\n",
    "        train_lbl_0 += torch.count_nonzero(1-labels)\n",
    "        train_lbl_1 += torch.count_nonzero(labels)\n",
    "        \n",
    "    print(f'Number of label 0 (fake) training images: {train_lbl_0}')\n",
    "    print(f'Number of label 1 (real) training images: {train_lbl_1}')\n",
    "\n",
    "    val_lbl_0 = 0\n",
    "    val_lbl_1 = 0\n",
    "    for _, _, _, labels in tqdm(val_dataloader):\n",
    "        val_lbl_0 += torch.count_nonzero(1-labels)\n",
    "        val_lbl_1 += torch.count_nonzero(labels)\n",
    "        \n",
    "    print(f'Number of label 0 (fake) validation images: {val_lbl_0}')\n",
    "    print(f'Number of label 1 (real) validation images: {val_lbl_1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "Trained Model loaded...\n",
      "History result loaded...\n",
      "Best model:\n",
      "Accuracy = 0.9098\n",
      "Precision = 0.9132\n",
      "Recall = 0.9549\n"
     ]
    }
   ],
   "source": [
    "model = EfficientNetWrapper()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.to(device)\n",
    "\n",
    "if os.path.exists(mdl_pth):\n",
    "    checkpoint = torch.load(mdl_pth)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    print(\"Trained Model loaded...\")\n",
    "else:\n",
    "    print(\"New Model created...\")\n",
    "    \n",
    "if os.path.exists(rst_pth):\n",
    "    checkpoint = torch.load(rst_pth)\n",
    "    epoch_train_loss = checkpoint['epoch_training_loss']\n",
    "    epoch_train_accuracy = checkpoint['epoch_training_accuracy']\n",
    "    epoch_train_precision = checkpoint['epoch_training_precision']\n",
    "    epoch_train_recall = checkpoint['epoch_train_recall']\n",
    "    epoch_val_loss = checkpoint['epoch_val_loss']\n",
    "    epoch_val_accuracy = checkpoint['epoch_val_accuracy']\n",
    "    epoch_val_precision = checkpoint['epoch_val_precision']\n",
    "    epoch_val_recall = checkpoint['epoch_val_recall']\n",
    "    best_val_acc = checkpoint['best_val_acc']\n",
    "    corres_val_precision = checkpoint['corres_val_precision']\n",
    "    corres_val_recall = checkpoint['corres_val_recall']\n",
    "    print(\"History result loaded...\")\n",
    "    print('Best model:')\n",
    "    print(f'Accuracy = {best_val_acc:.4f}')\n",
    "    print(f'Precision = {corres_val_precision:.4f}')\n",
    "    print(f'Recall = {corres_val_recall:.4f}')\n",
    "else:\n",
    "    epoch_train_loss = []\n",
    "    epoch_train_accuracy = []\n",
    "    epoch_train_precision = []\n",
    "    epoch_train_recall = []\n",
    "    epoch_val_loss = []\n",
    "    epoch_val_accuracy = []\n",
    "    epoch_val_precision = []\n",
    "    epoch_val_recall = []\n",
    "    best_val_acc = 0.0 \n",
    "    corres_val_precision = 0.0\n",
    "    corres_val_recall = 0.0\n",
    "    print(\"No history result...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [06:09<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5]\n",
      "Trn loss: 1.0147\n",
      "Trn accuracy: 0.9926\n",
      "Trn precision: 0.9926\n",
      "Trn recall: 0.9915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 212/212 [05:23<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.0001\n",
      "Val accuracy: 0.9196\n",
      "Val precision: 0.9221\n",
      "val recall: 0.9579\n",
      "Best model: Accuracy = 0.9196, Precision = 0.9221, Recall = 0.9579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [05:27<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5]\n",
      "Trn loss: 0.8701\n",
      "Trn accuracy: 0.9952\n",
      "Trn precision: 0.9952\n",
      "Trn recall: 0.9952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 212/212 [04:53<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.0001\n",
      "Val accuracy: 0.9235\n",
      "Val precision: 0.9247\n",
      "val recall: 0.9501\n",
      "Best model: Accuracy = 0.9235, Precision = 0.9247, Recall = 0.9501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [05:28<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5]\n",
      "Trn loss: 1.1013\n",
      "Trn accuracy: 0.9945\n",
      "Trn precision: 0.9945\n",
      "Trn recall: 0.9943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 212/212 [04:56<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.0002\n",
      "Val accuracy: 0.9236\n",
      "Val precision: 0.9245\n",
      "val recall: 0.9463\n",
      "Best model: Accuracy = 0.9236, Precision = 0.9245, Recall = 0.9463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [05:25<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5]\n",
      "Trn loss: 0.2137\n",
      "Trn accuracy: 0.9965\n",
      "Trn precision: 0.9965\n",
      "Trn recall: 0.9969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 212/212 [04:56<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.1811\n",
      "Val accuracy: 0.9248\n",
      "Val precision: 0.9264\n",
      "val recall: 0.9555\n",
      "Best model: Accuracy = 0.9248, Precision = 0.9264, Recall = 0.9555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [05:25<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5]\n",
      "Trn loss: 0.1916\n",
      "Trn accuracy: 0.9967\n",
      "Trn precision: 0.9967\n",
      "Trn recall: 0.9960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 212/212 [04:53<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.0008\n",
      "Val accuracy: 0.9313\n",
      "Val precision: 0.9315\n",
      "val recall: 0.9430\n",
      "Best model: Accuracy = 0.9313, Precision = 0.9315, Recall = 0.9430\n",
      "=========================================================\n",
      "Best model: Accuracy = 0.9313, Precision = 0.9315, Recall = 0.9430\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCH):\n",
    "    # ====================== train ===========================\n",
    "    train_all_labels = []\n",
    "    train_all_preds = []\n",
    "    model.train()\n",
    "    for rgb_data, hsv_data, lch_data, labels in tqdm(train_dataloader):\n",
    "        rgb_data, hsv_data, lch_data, labels = rgb_data.to(device), hsv_data.to(device), lch_data.to(device), labels.to(device)\n",
    "\n",
    "        hsv_data = hsv_data.reshape((rgb_data.shape)).float()\n",
    "        lch_data = lch_data.reshape((rgb_data.shape)).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(rgb_data, hsv_data, lch_data)\n",
    "        # hsv_output = model.hsv_model(hsv_data)\n",
    "        # lch_output = model.lch_model(lch_data)\n",
    "        \n",
    "        # concatenated_output = torch.cat([rgb_output, hsv_output, lch_output], dim=1)\n",
    "        \n",
    "        # outputs = model.fc(concatenated_output)\n",
    "        \n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        train_loss = criterion(outputs, labels)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_all_labels.extend(labels.cpu().numpy())\n",
    "        train_all_preds.extend(preds.cpu().numpy())\n",
    "        \n",
    "    train_accuracy = accuracy_score(train_all_labels, train_all_preds)\n",
    "    train_precision = precision_score(train_all_labels, train_all_preds, average='weighted')\n",
    "    train_recall = recall_score(train_all_labels, train_all_preds)\n",
    "        \n",
    "    epoch_train_loss += [train_loss.item()]\n",
    "    epoch_train_accuracy += [train_accuracy]\n",
    "    epoch_train_precision += [train_precision]\n",
    "    epoch_train_recall += [train_recall]\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{NUM_EPOCH}]')\n",
    "    print(f'Trn loss: {train_loss:.4f}')\n",
    "    print(f'Trn accuracy: {train_accuracy:.4f}')\n",
    "    print(f'Trn precision: {train_precision:.4f}')\n",
    "    print(f'Trn recall: {train_recall:.4f}')\n",
    "    \n",
    "    # ======================= val ============================\n",
    "    model.eval()\n",
    "    val_all_labels = []\n",
    "    val_all_preds = []\n",
    "    val_recall = []\n",
    "    with torch.no_grad():\n",
    "        for rgb_data, hsv_data, lch_data, labels in tqdm(val_dataloader):\n",
    "            rgb_data, hsv_data, lch_data, labels = rgb_data.to(device), hsv_data.to(device), lch_data.to(device), labels.to(device)\n",
    "\n",
    "            hsv_data = hsv_data.reshape((rgb_data.shape)).float()\n",
    "            lch_data = lch_data.reshape((rgb_data.shape)).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # rgb_output = model.rgb_model(rgb_data)\n",
    "            # hsv_output = model.hsv_model(hsv_data)\n",
    "            # lch_output = model.lch_model(lch_data)\n",
    "\n",
    "            # concatenated_output = torch.cat([rgb_output, hsv_output, lch_output], dim=1)\n",
    "            # outputs = model.fc(concatenated_output)\n",
    "            \n",
    "            outputs = model(rgb_data, hsv_data, lch_data)          \n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_all_labels.extend(labels.cpu().numpy())\n",
    "            val_all_preds.extend(preds.cpu().numpy())\n",
    "            \n",
    "            val_loss = criterion(outputs, labels)\n",
    "\n",
    "    val_accuracy = accuracy_score(val_all_labels, val_all_preds)\n",
    "    val_precision = precision_score(val_all_labels, val_all_preds, average='weighted')\n",
    "    val_recall = recall_score(val_all_labels, val_all_preds)\n",
    "    \n",
    "    epoch_val_loss += [val_loss.item()]\n",
    "    epoch_val_accuracy += [val_accuracy]\n",
    "    epoch_val_precision += [val_precision]\n",
    "    epoch_val_recall += [val_recall]\n",
    "    \n",
    "    print(f'Val loss: {val_loss:.4f}')\n",
    "    print(f'Val accuracy: {val_accuracy:.4f}')\n",
    "    print(f'Val precision: {val_precision:.4f}')\n",
    "    print(f'val recall: {val_recall:.4f}')\n",
    "        \n",
    "    if best_val_acc < val_accuracy:\n",
    "        best_val_acc = val_accuracy\n",
    "        corres_val_precision = val_precision\n",
    "        corres_val_recall = val_recall\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, mdl_pth)\n",
    "        \n",
    "    torch.save({\n",
    "        'epoch_training_loss': epoch_train_loss,\n",
    "        'epoch_training_accuracy': epoch_train_accuracy,\n",
    "        'epoch_training_precision': epoch_train_precision,\n",
    "        'epoch_train_recall': epoch_train_recall,\n",
    "        'epoch_val_loss': epoch_val_loss,\n",
    "        'epoch_val_accuracy': epoch_val_accuracy,\n",
    "        'epoch_val_precision': epoch_val_precision,\n",
    "        'epoch_val_recall': epoch_val_recall,\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'corres_val_precision': corres_val_precision,\n",
    "        'corres_val_recall': corres_val_recall\n",
    "    }, rst_pth)\n",
    "    \n",
    "    print(f'Best model: Accuracy = {best_val_acc:.4f}, Precision = {corres_val_precision:.4f}, Recall = {corres_val_recall:.4f}')\n",
    "print(\"=========================================================\")\n",
    "print(f'Best model: Accuracy = {best_val_acc:.4f}, Precision = {corres_val_precision:.4f}, Recall = {corres_val_recall:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
